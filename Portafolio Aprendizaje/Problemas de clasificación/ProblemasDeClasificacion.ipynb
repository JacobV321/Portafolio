{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Jacob Valdenegro Monzón A01640992"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZF-_9d2yWl__"
      },
      "source": [
        "# Ejercicio 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTYIpRVlMlqT"
      },
      "source": [
        "**Determina si es necesario balancear los datos.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WEr6s_N1MUZN",
        "outputId": "a577fe10-cba6-4c1d-ecba-14ea75bbe224"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Clase 1: 284 ejemplos\n",
            "Clase 2: 1706 ejemplos\n",
            "Clase 1: 14.27% del total\n",
            "Clase 2: 85.73% del total\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Cargar los datos desde el archivo\n",
        "data = np.loadtxt('P1_2.txt')\n",
        "\n",
        "y = data[:, 0]\n",
        "X = data[:, 1:]\n",
        "\n",
        "unicas, conteos = np.unique(y, return_counts=True)\n",
        "\n",
        "for clase, conteo in zip(unicas, conteos):\n",
        "    print(f'Clase {int(clase)}: {conteo} ejemplos')\n",
        "\n",
        "total_ejemplos = len(y)\n",
        "proporciones = conteos / total_ejemplos\n",
        "\n",
        "for clase, proporcion in zip(unicas, proporciones):\n",
        "    print(f'Clase {int(clase)}: {proporcion:.2%} del total')\n",
        "\n",
        "max_proporcion = np.max(proporciones)\n",
        "min_proporcion = np.min(proporciones)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Br0pGH9wVg-l",
        "outputId": "cd59e9ac-548a-45df-abd3-8ecf04ba99aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----- Upsampling -----\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         1.0       0.48      0.79      0.59       284\n",
            "         2.0       0.96      0.86      0.91      1706\n",
            "\n",
            "    accuracy                           0.85      1990\n",
            "   macro avg       0.72      0.82      0.75      1990\n",
            "weighted avg       0.89      0.85      0.86      1990\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Cargar conjunto de datos\n",
        "data = np.loadtxt(\"P1_2.txt\")\n",
        "\n",
        "x = data[:, 1:]\n",
        "y = data[:, 0]\n",
        "\n",
        "##### Precision con muestra balanceada (Upsampling) #####\n",
        "print(\"----- Upsampling -----\")\n",
        "\n",
        "# Definir el clasificador\n",
        "clf = SVC(kernel='linear')\n",
        "\n",
        "# Configurar validacion cruzada estratificada\n",
        "kf = StratifiedKFold(n_splits=5, shuffle=True)\n",
        "\n",
        "cv_y_test = []\n",
        "cv_y_pred = []\n",
        "\n",
        "for train_index, test_index in kf.split(x, y):\n",
        "    # Fase de entrenamiento\n",
        "    x_train = x[train_index, :]\n",
        "    y_train = y[train_index]\n",
        "\n",
        "    x1 = x_train[y_train == 1, :]\n",
        "    y1 = y_train[y_train == 1]\n",
        "    n1 = len(y1)\n",
        "\n",
        "    x2 = x_train[y_train == 2, :]\n",
        "    y2 = y_train[y_train == 2]\n",
        "    n2 = len(y2)\n",
        "\n",
        "    # Realizar upsampling\n",
        "    ind = random.choices([i for i in range(n1)], k=n2)\n",
        "    x_sub = np.concatenate((x1[ind, :], x2), axis=0)\n",
        "    y_sub = np.concatenate((y1[ind], y2), axis=0)\n",
        "\n",
        "    # Entrenar el clasificador\n",
        "    clf.fit(x_sub, y_sub)\n",
        "\n",
        "    # Fase de prueba\n",
        "    x_test = x[test_index, :]\n",
        "    y_test = y[test_index]\n",
        "    y_pred = clf.predict(x_test)\n",
        "\n",
        "    cv_y_test.append(y_test)\n",
        "    cv_y_pred.append(y_pred)\n",
        "\n",
        "# Mostrar el informe de clasificacion\n",
        "print(classification_report(np.concatenate(cv_y_test), np.concatenate(cv_y_pred)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evalúa al menos 8 modelos de clasificación distintos utilizando validación cruzada, y determina cuál de ellos es el más efectivo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----- SVC -----\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         1.0       0.48      0.80      0.60       284\n",
            "         2.0       0.96      0.86      0.91      1706\n",
            "\n",
            "    accuracy                           0.85      1990\n",
            "   macro avg       0.72      0.83      0.75      1990\n",
            "weighted avg       0.89      0.85      0.86      1990\n",
            "\n",
            "----- Random Forest -----\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         1.0       0.97      0.22      0.36       284\n",
            "         2.0       0.88      1.00      0.94      1706\n",
            "\n",
            "    accuracy                           0.89      1990\n",
            "   macro avg       0.93      0.61      0.65      1990\n",
            "weighted avg       0.90      0.89      0.86      1990\n",
            "\n",
            "----- KNN -----\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         1.0       0.34      0.62      0.44       284\n",
            "         2.0       0.93      0.80      0.86      1706\n",
            "\n",
            "    accuracy                           0.78      1990\n",
            "   macro avg       0.64      0.71      0.65      1990\n",
            "weighted avg       0.84      0.78      0.80      1990\n",
            "\n",
            "----- Logistic Regression -----\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         1.0       0.49      0.78      0.60       284\n",
            "         2.0       0.96      0.86      0.91      1706\n",
            "\n",
            "    accuracy                           0.85      1990\n",
            "   macro avg       0.72      0.82      0.75      1990\n",
            "weighted avg       0.89      0.85      0.86      1990\n",
            "\n",
            "----- Decision Tree -----\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         1.0       0.36      0.35      0.35       284\n",
            "         2.0       0.89      0.90      0.90      1706\n",
            "\n",
            "    accuracy                           0.82      1990\n",
            "   macro avg       0.63      0.62      0.62      1990\n",
            "weighted avg       0.82      0.82      0.82      1990\n",
            "\n",
            "----- Naive Bayes -----\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         1.0       0.49      0.75      0.59       284\n",
            "         2.0       0.95      0.87      0.91      1706\n",
            "\n",
            "    accuracy                           0.85      1990\n",
            "   macro avg       0.72      0.81      0.75      1990\n",
            "weighted avg       0.89      0.85      0.87      1990\n",
            "\n",
            "----- Gradient Boosting -----\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         1.0       0.66      0.58      0.62       284\n",
            "         2.0       0.93      0.95      0.94      1706\n",
            "\n",
            "    accuracy                           0.90      1990\n",
            "   macro avg       0.80      0.77      0.78      1990\n",
            "weighted avg       0.89      0.90      0.90      1990\n",
            "\n",
            "----- AdaBoost -----\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\jacob\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jacob\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jacob\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jacob\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jacob\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         1.0       0.53      0.67      0.59       284\n",
            "         2.0       0.94      0.90      0.92      1706\n",
            "\n",
            "    accuracy                           0.87      1990\n",
            "   macro avg       0.74      0.79      0.76      1990\n",
            "weighted avg       0.88      0.87      0.87      1990\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Cargar el conjunto de datos\n",
        "data = np.loadtxt(\"P1_2.txt\")\n",
        "x = data[:, 1:]\n",
        "y = data[:, 0]\n",
        "\n",
        "# Lista de modelos a evaluar\n",
        "models = {\n",
        "    'SVC': SVC(kernel='linear'),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'KNN': KNeighborsClassifier(),\n",
        "    'Logistic Regression': LogisticRegression(),\n",
        "    'Decision Tree': DecisionTreeClassifier(),\n",
        "    'Naive Bayes': GaussianNB(),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(),\n",
        "    'AdaBoost': AdaBoostClassifier()\n",
        "}\n",
        "\n",
        "# Validacion cruzada y evaluacion con muestra balanceada (Upsampling)\n",
        "for model_name, model in models.items():\n",
        "    print(f\"----- {model_name} -----\")\n",
        "    \n",
        "    kf = StratifiedKFold(n_splits=5, shuffle=True)\n",
        "    \n",
        "    cv_y_test = []\n",
        "    cv_y_pred = []\n",
        "    \n",
        "    for train_index, test_index in kf.split(x, y):\n",
        "        # Fase de entrenamiento\n",
        "        x_train = x[train_index, :]\n",
        "        y_train = y[train_index]\n",
        "        \n",
        "        x1 = x_train[y_train == 1, :]\n",
        "        y1 = y_train[y_train == 1]\n",
        "        n1 = len(y1)\n",
        "        \n",
        "        x2 = x_train[y_train == 2, :]\n",
        "        y2 = y_train[y_train == 2]\n",
        "        n2 = len(y2)\n",
        "        \n",
        "        ind = random.choices([i for i in range(n1)], k=n2)\n",
        "        \n",
        "        x_sub = np.concatenate((x1[ind, :], x2), axis=0)\n",
        "        y_sub = np.concatenate((y1[ind], y2), axis=0)\n",
        "        \n",
        "        model.fit(x_sub, y_sub)\n",
        "        \n",
        "        # Fase de prueba\n",
        "        x_test = x[test_index, :]\n",
        "        y_test = y[test_index]\n",
        "        y_pred = model.predict(x_test)\n",
        "        \n",
        "        cv_y_test.append(y_test)\n",
        "        cv_y_pred.append(y_pred)\n",
        "    \n",
        "    print(classification_report(np.concatenate(cv_y_test), np.concatenate(cv_y_pred)))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Implementa desde cero el método de regresión logística, y evalúalo con el conjunto de datos. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----- Regresión Logística -----\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         1.0       0.14      0.99      0.25       284\n",
            "         2.0       0.84      0.01      0.02      1706\n",
            "\n",
            "    accuracy                           0.15      1990\n",
            "   macro avg       0.49      0.50      0.14      1990\n",
            "weighted avg       0.74      0.15      0.06      1990\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "class RegresionLogisticaDesdeCero:\n",
        "    def __init__(self, tasa_aprendizaje=0.01, num_iteraciones=1000):\n",
        "        self.tasa_aprendizaje = tasa_aprendizaje\n",
        "        self.num_iteraciones = num_iteraciones\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        num_muestras, num_caracteristicas = X.shape\n",
        "        self.theta = np.zeros(num_caracteristicas)\n",
        "        self.intercepto = 0\n",
        "\n",
        "        for _ in range(self.num_iteraciones):\n",
        "            modelo = np.dot(X, self.theta) + self.intercepto\n",
        "            predicciones = self.sigmoid(modelo)\n",
        "\n",
        "            # Calculo del gradiente\n",
        "            errores = y - predicciones\n",
        "            gradiente_theta = -np.dot(X.T, errores) / num_muestras\n",
        "            gradiente_intercepto = -np.sum(errores) / num_muestras\n",
        "\n",
        "            # Actualizacion de los parametros\n",
        "            self.theta -= self.tasa_aprendizaje * gradiente_theta\n",
        "            self.intercepto -= self.tasa_aprendizaje * gradiente_intercepto\n",
        "\n",
        "    def predecir_prob(self, X):\n",
        "        modelo = np.dot(X, self.theta) + self.intercepto\n",
        "        return self.sigmoid(modelo)\n",
        "\n",
        "    def predecir(self, X):\n",
        "        probabilidades = self.predecir_prob(X)\n",
        "        return np.where(probabilidades >= 0.5, 1, 2)  # Asumiendo clasificacion binaria\n",
        "\n",
        "# Cargar conjunto de datos\n",
        "data = np.loadtxt(\"P1_2.txt\")\n",
        "\n",
        "x = data[:, 1:]\n",
        "y = data[:, 0]\n",
        "\n",
        "# Precision con muestra balanceada (Upsampling) para Regresion Logistica\n",
        "print(\"----- Regresion Logistica -----\")\n",
        "\n",
        "kf = StratifiedKFold(n_splits=5, shuffle=True)\n",
        "\n",
        "cv_y_test = []\n",
        "cv_y_pred = []\n",
        "\n",
        "for train_index, test_index in kf.split(x, y):\n",
        "    # Fase de entrenamiento\n",
        "    x_train = x[train_index, :]\n",
        "    y_train = y[train_index]\n",
        "\n",
        "    x1 = x_train[y_train == 1, :]\n",
        "    y1 = y_train[y_train == 1]\n",
        "    n1 = len(y1)\n",
        "\n",
        "    x2 = x_train[y_train == 2, :]\n",
        "    y2 = y_train[y_train == 2]\n",
        "    n2 = len(y2)\n",
        "\n",
        "    ind = random.choices([i for i in range(n1)], k=n2)\n",
        "\n",
        "    x_sub = np.concatenate((x1[ind, :], x2), axis=0)\n",
        "    y_sub = np.concatenate((y1[ind], y2), axis=0)\n",
        "\n",
        "    # Entrenar el modelo\n",
        "    clf = RegresionLogisticaDesdeCero(tasa_aprendizaje=0.01, num_iteraciones=1000)\n",
        "    clf.fit(x_sub, y_sub)\n",
        "\n",
        "    # Fase de prueba\n",
        "    x_test = x[test_index, :]\n",
        "    y_test = y[test_index]\n",
        "    y_pred = clf.predecir(x_test)\n",
        "\n",
        "    cv_y_test.append(y_test)\n",
        "    cv_y_pred.append(y_pred)\n",
        "\n",
        "print(classification_report(np.concatenate(cv_y_test), np.concatenate(cv_y_pred)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Con alguno de los clasificadores que probaste en los pasos anteriores, determina el número óptimo de características utilizando un método tipo Filter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Número de características: 1, Precisión Media: 0.6146\n",
            "Número de características: 2, Precisión Media: 0.6558\n",
            "Número de características: 3, Precisión Media: 0.6854\n",
            "Número de características: 4, Precisión Media: 0.7045\n",
            "Número de características: 5, Precisión Media: 0.7141\n",
            "Número de características: 6, Precisión Media: 0.7332\n",
            "Número de características: 7, Precisión Media: 0.7548\n",
            "Número de características: 8, Precisión Media: 0.7618\n",
            "Número de características: 9, Precisión Media: 0.7824\n",
            "Número de características: 10, Precisión Media: 0.7714\n",
            "Número de características: 11, Precisión Media: 0.8030\n",
            "Número de características: 12, Precisión Media: 0.7834\n",
            "Número de características: 13, Precisión Media: 0.7925\n",
            "Número de características: 14, Precisión Media: 0.8055\n",
            "Número de características: 15, Precisión Media: 0.8116\n",
            "Número de características: 16, Precisión Media: 0.8075\n",
            "Número de características: 17, Precisión Media: 0.8196\n",
            "Número de características: 18, Precisión Media: 0.8186\n",
            "Número de características: 19, Precisión Media: 0.8151\n",
            "Número de características: 20, Precisión Media: 0.8271\n",
            "Número de características: 21, Precisión Media: 0.8302\n",
            "Número de características: 22, Precisión Media: 0.8377\n",
            "Número de características: 23, Precisión Media: 0.8337\n",
            "Número de características: 24, Precisión Media: 0.8397\n",
            "Número de características: 25, Precisión Media: 0.8462\n",
            "Número de características: 26, Precisión Media: 0.8417\n",
            "Número de características: 27, Precisión Media: 0.8332\n",
            "Número de características: 28, Precisión Media: 0.8397\n",
            "Número de características: 29, Precisión Media: 0.8447\n",
            "Número de características: 30, Precisión Media: 0.8432\n",
            "Número de características: 31, Precisión Media: 0.8492\n",
            "Número de características: 32, Precisión Media: 0.8387\n",
            "Número de características: 33, Precisión Media: 0.8432\n",
            "Número de características: 34, Precisión Media: 0.8377\n",
            "Número de características: 35, Precisión Media: 0.8548\n",
            "Número de características: 36, Precisión Media: 0.8472\n",
            "Número de características: 37, Precisión Media: 0.8442\n",
            "Número de características: 38, Precisión Media: 0.8492\n",
            "Número de características: 39, Precisión Media: 0.8528\n",
            "Número de características: 40, Precisión Media: 0.8588\n",
            "Número de características: 41, Precisión Media: 0.8457\n",
            "Número de características: 42, Precisión Media: 0.8528\n",
            "Número de características: 43, Precisión Media: 0.8467\n",
            "Número de características: 44, Precisión Media: 0.8583\n",
            "Número de características: 45, Precisión Media: 0.8497\n",
            "Número de características: 46, Precisión Media: 0.8583\n",
            "Número de características: 47, Precisión Media: 0.8513\n",
            "Número de características: 48, Precisión Media: 0.8508\n",
            "Número de características: 49, Precisión Media: 0.8462\n",
            "Número de características: 50, Precisión Media: 0.8437\n",
            "Número de características: 51, Precisión Media: 0.8442\n",
            "Número de características: 52, Precisión Media: 0.8482\n",
            "Número de características: 53, Precisión Media: 0.8487\n",
            "Número de características: 54, Precisión Media: 0.8412\n",
            "Número de características: 55, Precisión Media: 0.8492\n",
            "Número de características: 56, Precisión Media: 0.8472\n",
            "Número de características: 57, Precisión Media: 0.8457\n",
            "Número de características: 58, Precisión Media: 0.8452\n",
            "Número de características: 59, Precisión Media: 0.8492\n",
            "Número de características: 60, Precisión Media: 0.8528\n",
            "Número de características: 61, Precisión Media: 0.8482\n",
            "Número de características: 62, Precisión Media: 0.8543\n",
            "Número de características: 63, Precisión Media: 0.8487\n",
            "Número de características: 64, Precisión Media: 0.8492\n",
            "Número de características: 65, Precisión Media: 0.8392\n",
            "Número de características: 66, Precisión Media: 0.8482\n",
            "Número de características: 67, Precisión Media: 0.8462\n",
            "Número de características: 68, Precisión Media: 0.8427\n",
            "Número de características: 69, Precisión Media: 0.8492\n",
            "Número de características: 70, Precisión Media: 0.8543\n",
            "Número de características: 71, Precisión Media: 0.8503\n",
            "Número de características: 72, Precisión Media: 0.8402\n",
            "Número de características: 73, Precisión Media: 0.8508\n",
            "Número de características: 74, Precisión Media: 0.8518\n",
            "Número de características: 75, Precisión Media: 0.8442\n",
            "Número de características: 76, Precisión Media: 0.8568\n",
            "Número de características: 77, Precisión Media: 0.8503\n",
            "Número de características: 78, Precisión Media: 0.8482\n",
            "Número de características: 79, Precisión Media: 0.8467\n",
            "Número de características: 80, Precisión Media: 0.8492\n",
            "Número de características: 81, Precisión Media: 0.8452\n",
            "Número de características: 82, Precisión Media: 0.8553\n",
            "Número de características: 83, Precisión Media: 0.8487\n",
            "Número de características: 84, Precisión Media: 0.8573\n",
            "Número de características: 85, Precisión Media: 0.8513\n",
            "Número de características: 86, Precisión Media: 0.8487\n",
            "Número de características: 87, Precisión Media: 0.8538\n",
            "Número de características: 88, Precisión Media: 0.8467\n",
            "Número de características: 89, Precisión Media: 0.8513\n",
            "Número de características: 90, Precisión Media: 0.8543\n",
            "Número de características: 91, Precisión Media: 0.8578\n",
            "Número de características: 92, Precisión Media: 0.8447\n",
            "Número de características: 93, Precisión Media: 0.8437\n",
            "Número de características: 94, Precisión Media: 0.8412\n",
            "Número de características: 95, Precisión Media: 0.8568\n",
            "Número de características: 96, Precisión Media: 0.8553\n",
            "Número de características: 97, Precisión Media: 0.8613\n",
            "Número de características: 98, Precisión Media: 0.8508\n",
            "Número de características: 99, Precisión Media: 0.8608\n",
            "Número de características: 100, Precisión Media: 0.8447\n",
            "Número de características: 101, Precisión Media: 0.8523\n",
            "Número de características: 102, Precisión Media: 0.8563\n",
            "Número de características: 103, Precisión Media: 0.8523\n",
            "Número de características: 104, Precisión Media: 0.8583\n",
            "Número de características: 105, Precisión Media: 0.8497\n",
            "Número de características: 106, Precisión Media: 0.8477\n",
            "Número de características: 107, Precisión Media: 0.8628\n",
            "Número de características: 108, Precisión Media: 0.8417\n",
            "Número de características: 109, Precisión Media: 0.8598\n",
            "Número de características: 110, Precisión Media: 0.8497\n",
            "Número de características: 111, Precisión Media: 0.8553\n",
            "Número de características: 112, Precisión Media: 0.8563\n",
            "Número de características: 113, Precisión Media: 0.8538\n",
            "Número de características: 114, Precisión Media: 0.8477\n",
            "Número de características: 115, Precisión Media: 0.8492\n",
            "Número de características: 116, Precisión Media: 0.8513\n",
            "Número de características: 117, Precisión Media: 0.8477\n",
            "Número de características: 118, Precisión Media: 0.8472\n",
            "Número de características: 119, Precisión Media: 0.8558\n",
            "Número de características: 120, Precisión Media: 0.8673\n",
            "Número de características: 121, Precisión Media: 0.8442\n",
            "Número de características: 122, Precisión Media: 0.8472\n",
            "Número de características: 123, Precisión Media: 0.8457\n",
            "Número de características: 124, Precisión Media: 0.8528\n",
            "Número de características: 125, Precisión Media: 0.8412\n",
            "Número de características: 126, Precisión Media: 0.8472\n",
            "Número de características: 127, Precisión Media: 0.8447\n",
            "Número de características: 128, Precisión Media: 0.8417\n",
            "Número de características: 129, Precisión Media: 0.8442\n",
            "Número de características: 130, Precisión Media: 0.8462\n",
            "Número de características: 131, Precisión Media: 0.8432\n",
            "Número de características: 132, Precisión Media: 0.8492\n",
            "Número de características: 133, Precisión Media: 0.8407\n",
            "Número de características: 134, Precisión Media: 0.8422\n",
            "Número de características: 135, Precisión Media: 0.8543\n",
            "Número de características: 136, Precisión Media: 0.8558\n",
            "Número de características: 137, Precisión Media: 0.8543\n",
            "Número de características: 138, Precisión Media: 0.8538\n",
            "Número de características: 139, Precisión Media: 0.8503\n",
            "Número de características: 140, Precisión Media: 0.8417\n",
            "Número de características: 141, Precisión Media: 0.8528\n",
            "Número de características: 142, Precisión Media: 0.8523\n",
            "Número de características: 143, Precisión Media: 0.8543\n",
            "Número de características: 144, Precisión Media: 0.8533\n",
            "Número de características: 145, Precisión Media: 0.8487\n",
            "Número de características: 146, Precisión Media: 0.8467\n",
            "Número de características: 147, Precisión Media: 0.8392\n",
            "Número de características: 148, Precisión Media: 0.8447\n",
            "Número de características: 149, Precisión Media: 0.8508\n",
            "Número de características: 150, Precisión Media: 0.8457\n",
            "Número de características: 151, Precisión Media: 0.8452\n",
            "Número de características: 152, Precisión Media: 0.8422\n",
            "Número de características: 153, Precisión Media: 0.8467\n",
            "Número de características: 154, Precisión Media: 0.8528\n",
            "Número óptimo de características: 120 con precisión 0.8673\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Cargar el conjunto de datos\n",
        "data = np.loadtxt(\"P1_2.txt\")\n",
        "x = data[:, 1:]\n",
        "y = data[:, 0]\n",
        "\n",
        "# Definir funcion para realizar upsampling y evaluar el modelo\n",
        "def evaluate_model(x_train, y_train, x_test, y_test, k_features):\n",
        "    # Aplicar upsampling\n",
        "    x1 = x_train[y_train == 1, :]\n",
        "    y1 = y_train[y_train == 1]\n",
        "    n1 = len(y1)\n",
        "\n",
        "    x2 = x_train[y_train == 2, :]\n",
        "    y2 = y_train[y_train == 2]\n",
        "    n2 = len(y2)\n",
        "\n",
        "    ind = random.choices([i for i in range(n1)], k=n2)\n",
        "\n",
        "    x_sub = np.concatenate((x1[ind, :], x2), axis=0)\n",
        "    y_sub = np.concatenate((y1[ind], y2), axis=0)\n",
        "\n",
        "    # Seleccion de caracteristicas\n",
        "    selector = SelectKBest(f_classif, k=k_features)\n",
        "    x_sub_selected = selector.fit_transform(x_sub, y_sub)\n",
        "    x_test_selected = selector.transform(x_test)\n",
        "\n",
        "    # Entrenamiento del modelo\n",
        "    clf = SVC(kernel='linear')\n",
        "    clf.fit(x_sub_selected, y_sub)\n",
        "\n",
        "    # Fase de prueba\n",
        "    y_pred = clf.predict(x_test_selected)\n",
        "\n",
        "    return classification_report(y_test, y_pred, output_dict=True)\n",
        "\n",
        "# Definir parametros\n",
        "kf = StratifiedKFold(n_splits=5, shuffle=True)\n",
        "k_features_range = range(1, x.shape[1] + 1)\n",
        "results = {}\n",
        "\n",
        "# Realizar validacion cruzada y seleccion de caracteristicas\n",
        "for k_features in k_features_range:\n",
        "    fold_reports = []\n",
        "    for train_index, test_index in kf.split(x, y):\n",
        "        x_train, x_test = x[train_index, :], x[test_index, :]\n",
        "        y_train, y_test = y[train_index], y[test_index]\n",
        "        report = evaluate_model(x_train, y_train, x_test, y_test, k_features)\n",
        "        fold_reports.append(report['accuracy'])\n",
        "\n",
        "    # Calcular la precision media para el numero actual de caracteristicas\n",
        "    average_accuracy = np.mean(fold_reports)\n",
        "    results[k_features] = average_accuracy\n",
        "\n",
        "# Imprimir resultados\n",
        "for k_features, accuracy in results.items():\n",
        "    print(f\"Numero de caracteristicas: {k_features}, Precision Media: {accuracy:.4f}\")\n",
        "\n",
        "# Determinar el numero optimo de caracteristicas\n",
        "optimal_k = max(results, key=results.get)\n",
        "print(f\"Numero optimo de caracteristicas: {optimal_k} con precision {results[optimal_k]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Repite el paso anterior, pero para un método de selección de características secuencial.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_selection import SequentialFeatureSelector\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Cargar el conjunto de datos\n",
        "data = np.loadtxt(\"P1_2.txt\")\n",
        "x = data[:, 1:]\n",
        "y = data[:, 0]\n",
        "\n",
        "# Definir funcion para realizar upsampling y evaluar el modelo\n",
        "def evaluate_model(x_train, y_train, x_test, y_test, n_features):\n",
        "    # Aplicar upsampling\n",
        "    x1 = x_train[y_train == 1, :]\n",
        "    y1 = y_train[y_train == 1]\n",
        "    n1 = len(y1)\n",
        "\n",
        "    x2 = x_train[y_train == 2, :]\n",
        "    y2 = y_train[y_train == 2]\n",
        "    n2 = len(y2)\n",
        "\n",
        "    ind = random.choices([i for i in range(n1)], k=n2)\n",
        "\n",
        "    x_sub = np.concatenate((x1[ind, :], x2), axis=0)\n",
        "    y_sub = np.concatenate((y1[ind], y2), axis=0)\n",
        "\n",
        "    # Seleccion secuencial de características\n",
        "    selector = SequentialFeatureSelector(SVC(kernel='linear'), n_features_to_select=n_features, direction='forward')\n",
        "    x_sub_selected = selector.fit_transform(x_sub, y_sub)\n",
        "    x_test_selected = selector.transform(x_test)\n",
        "\n",
        "    # Entrenamiento del modelo\n",
        "    clf = SVC(kernel='linear')\n",
        "    clf.fit(x_sub_selected, y_sub)\n",
        "\n",
        "    # Fase de prueba\n",
        "    y_pred = clf.predict(x_test_selected)\n",
        "\n",
        "    return classification_report(y_test, y_pred, output_dict=True)\n",
        "\n",
        "# Definir parametros\n",
        "kf = StratifiedKFold(n_splits=5, shuffle=True)\n",
        "n_features_range = range(1, x.shape[1] + 1)\n",
        "results = {}\n",
        "\n",
        "# Realizar validacion cruzada y seleccion de caracteristicas secuencial\n",
        "for n_features in n_features_range:\n",
        "    fold_reports = []\n",
        "    for train_index, test_index in kf.split(x, y):\n",
        "        x_train, x_test = x[train_index, :], x[test_index, :]\n",
        "        y_train, y_test = y[train_index], y[test_index]\n",
        "        report = evaluate_model(x_train, y_train, x_test, y_test, n_features)\n",
        "        fold_reports.append(report['accuracy'])\n",
        "\n",
        "    # Calcular la precision media para el numero actual de caracteristicas\n",
        "    average_accuracy = np.mean(fold_reports)\n",
        "    results[n_features] = average_accuracy\n",
        "\n",
        "# Imprimir resultados\n",
        "for n_features, accuracy in results.items():\n",
        "    print(f\"Numero de caracteristicas: {n_features}, Precision Media: {accuracy:.4f}\")\n",
        "\n",
        "# Determinar el numero optimo de caracteristicas\n",
        "optimal_n = max(results, key=results.get)\n",
        "print(f\"Número optimo de caracteristicas: {optimal_n} con precision {results[optimal_n]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tardo mas de 2 horas y media"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Repite el paso 4, pero para un método de selección de características recursivo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Cargar conjunto de datos\n",
        "data = np.loadtxt(\"P1_2.txt\")\n",
        "x = data[:, 1:]\n",
        "y = data[:, 0]\n",
        "\n",
        "# Definir funcion para realizar upsampling y evaluar el modelo\n",
        "def evaluate_model(x_train, y_train, x_test, y_test, n_features):\n",
        "    # Aplicar upsampling\n",
        "    x1 = x_train[y_train == 1, :]\n",
        "    y1 = y_train[y_train == 1]\n",
        "    n1 = len(y1)\n",
        "\n",
        "    x2 = x_train[y_train == 2, :]\n",
        "    y2 = y_train[y_train == 2]\n",
        "    n2 = len(y2)\n",
        "\n",
        "    ind = random.choices([i for i in range(n1)], k=n2)\n",
        "\n",
        "    x_sub = np.concatenate((x1[ind, :], x2), axis=0)\n",
        "    y_sub = np.concatenate((y1[ind], y2), axis=0)\n",
        "\n",
        "    # Seleccion de caracteristicas con RFE\n",
        "    clf = SVC(kernel='linear')\n",
        "    selector = RFE(clf, n_features_to_select=n_features, step=1)\n",
        "    x_sub_selected = selector.fit_transform(x_sub, y_sub)\n",
        "    x_test_selected = selector.transform(x_test)\n",
        "\n",
        "    # Entrenamiento del modelo\n",
        "    clf.fit(x_sub_selected, y_sub)\n",
        "\n",
        "    # Fase de prueba\n",
        "    y_pred = clf.predict(x_test_selected)\n",
        "\n",
        "    return classification_report(y_test, y_pred, output_dict=True)\n",
        "\n",
        "# Definir parametros\n",
        "kf = StratifiedKFold(n_splits=5, shuffle=True)\n",
        "n_features_range = range(1, x.shape[1] + 1)\n",
        "results = {}\n",
        "\n",
        "# Realizar validacion cruzada y seleccion de caracteristicas\n",
        "for n_features in n_features_range:\n",
        "    fold_reports = []\n",
        "    for train_index, test_index in kf.split(x, y):\n",
        "        x_train, x_test = x[train_index, :], x[test_index, :]\n",
        "        y_train, y_test = y[train_index], y[test_index]\n",
        "        report = evaluate_model(x_train, y_train, x_test, y_test, n_features)\n",
        "        fold_reports.append(report['accuracy'])\n",
        "\n",
        "    # Calcular la precision promedio para el numero actual de caracteristicas\n",
        "    average_accuracy = np.mean(fold_reports)\n",
        "    results[n_features] = average_accuracy\n",
        "\n",
        "# Imprimir resultados\n",
        "for n_features, accuracy in results.items():\n",
        "    print(f\"Numero de caracteristicas: {n_features}, Precision promedio: {accuracy:.4f}\")\n",
        "\n",
        "# Determinar el numero optimo de caracteristicas\n",
        "optimal_n = max(results, key=results.get)\n",
        "print(f\"Numero optimo de caracteristicas: {optimal_n} con precision {results[optimal_n]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tardo mas de 2 horas y media"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Escoge alguna de las técnicas de selección de características que probaste con anteioridad, y con el número óptimo de características encontrado, prepara tu modelo para producción haciendo lo siguiente:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modelo final ajustado con las 120 características más relevantes.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\jacob\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:776: UserWarning: k=120 is greater than n_features=10. All the features will be returned.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Seleccionar el numero optimo de caracteristicas\n",
        "k_optimal = 120\n",
        "\n",
        "# Aplicar seleccion de caracteristicas en todo el conjunto de datos\n",
        "selector = SelectKBest(f_classif, k=k_optimal)\n",
        "x_selected = selector.fit_transform(x, y)\n",
        "\n",
        "# Ajustar el modelo con todas las caracteristicas seleccionadas\n",
        "clf_final = SVC(kernel='linear')\n",
        "clf_final.fit(x_selected, y)\n",
        "\n",
        "print(f\"Modelo final ajustado con las {k_optimal} características mas relevantes.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**¿Qué pasa si no se considera el problema de tener datos desbalanceados para este caso? ¿Por qué?**\n",
        "\n",
        "El modelo puede mostrar una alta precisión global simplemente porque predice mayoritariamente la clase mayoritaria.\n",
        "\n",
        "**De todos los clasificadores, ¿cuál o cuales consideras que son adecuados para los datos? ¿Qué propiedades tienen dichos modelos que los hacen apropiados para los datos? Argumenta tu respuesta.**\n",
        "\n",
        "SVC, Gradient Boosting, y AdaBoost parecen ser los más prometedores debido a su capacidad para manejar el desbalanceo y ofrecer un rendimiento aceptable.\n",
        "\n",
        "**¿Es posibles reducir la dimensionalidad del problema sin perder rendimiento en el modelo?**\n",
        "\n",
        "Puede ser posble si se eliminan las caracteristicas relevantes.\n",
        "\n",
        "**¿Qué método de selección de características consideras el más adecuado para este caso? ¿Por qué?**\n",
        "\n",
        "El que use fue el filter ya que es rapido y facil de implementar y tambien ayuda a reducir la tamaño seleccionando las caracteristicas mas relevantes. puede que otros metodos sean mas precisos pero son mas costosos computacionalmente.\n",
        "\n",
        "**Si quisieras mejorar el rendimiento de tus modelos, ¿qué más se podría hacer?**\n",
        "\n",
        "Ajustar con hiperparametros, balancear los datos con otras tecnicas como SMOTE o combinar modelos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ejercicio 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Determina si es necesario balancear los datos. En caso de que sea afirmativo, en todo este ejercicio tendrás que utilizar alguna estrategia para mitigar el problema de tener una muestra desbalanceada.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Distribución de clases:\n",
            "Clase 1.0: 90 instancias\n",
            "Clase 2.0: 90 instancias\n",
            "Clase 3.0: 90 instancias\n",
            "Clase 4.0: 90 instancias\n",
            "Clase 5.0: 90 instancias\n",
            "Clase 6.0: 90 instancias\n",
            "Clase 7.0: 89 instancias\n",
            "Los datos están balanceados.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import collections\n",
        "\n",
        "# Cargar los datos, ignorando la segunda columna\n",
        "datos = np.loadtxt('M_5.txt', delimiter='\\t', usecols=[0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11])\n",
        "\n",
        "# Contar las instancias por clase\n",
        "clases = datos[:, 0]  # Primera columna son las clases\n",
        "conteo_clases = collections.Counter(clases)\n",
        "\n",
        "# Mostrar la distribucion de clases\n",
        "print(\"Distribucion de clases:\")\n",
        "for clase, conteo in conteo_clases.items():\n",
        "    print(f\"Clase {clase}: {conteo} instancias\")\n",
        "\n",
        "# Evaluar si es necesario balancear\n",
        "# Por ejemplo, si una clase tiene menos del 10% de las instancias totales, podria considerarse desbalanceada\n",
        "total_instancias = sum(conteo_clases.values())\n",
        "umbral = 0.10 * total_instancias\n",
        "\n",
        "clases_desbalanceadas = [clase for clase, conteo in conteo_clases.items() if conteo < umbral]\n",
        "\n",
        "if clases_desbalanceadas:\n",
        "    print(f\"Clases desbalanceadas: {clases_desbalanceadas}. Se recomienda balancear los datos.\")\n",
        "else:\n",
        "    print(\"Los datos estan balanceados.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evalúa al menos 8 modelos de clasificación distintos utilizando validación cruzada, y determina cuál de ellos es el más efectivo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----- SVC -----\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         1.0       0.77      0.71      0.74        90\n",
            "         2.0       0.51      0.49      0.50        90\n",
            "         3.0       0.98      0.94      0.96        90\n",
            "         4.0       0.86      0.83      0.85        90\n",
            "         5.0       0.64      0.66      0.65        90\n",
            "         6.0       0.58      0.67      0.62        90\n",
            "         7.0       0.98      1.00      0.99        89\n",
            "\n",
            "    accuracy                           0.76       629\n",
            "   macro avg       0.76      0.76      0.76       629\n",
            "weighted avg       0.76      0.76      0.76       629\n",
            "\n",
            "----- Random Forest -----\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         1.0       0.71      0.67      0.69        90\n",
            "         2.0       0.47      0.50      0.48        90\n",
            "         3.0       0.93      0.93      0.93        90\n",
            "         4.0       0.83      0.79      0.81        90\n",
            "         5.0       0.63      0.67      0.65        90\n",
            "         6.0       0.56      0.54      0.55        90\n",
            "         7.0       0.98      1.00      0.99        89\n",
            "\n",
            "    accuracy                           0.73       629\n",
            "   macro avg       0.73      0.73      0.73       629\n",
            "weighted avg       0.73      0.73      0.73       629\n",
            "\n",
            "----- KNN -----\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         1.0       0.72      0.66      0.69        90\n",
            "         2.0       0.41      0.57      0.48        90\n",
            "         3.0       0.94      0.91      0.93        90\n",
            "         4.0       0.76      0.76      0.76        90\n",
            "         5.0       0.64      0.59      0.61        90\n",
            "         6.0       0.58      0.48      0.52        90\n",
            "         7.0       0.98      1.00      0.99        89\n",
            "\n",
            "    accuracy                           0.71       629\n",
            "   macro avg       0.72      0.71      0.71       629\n",
            "weighted avg       0.72      0.71      0.71       629\n",
            "\n",
            "----- Logistic Regression -----\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         1.0       0.71      0.69      0.70        90\n",
            "         2.0       0.52      0.49      0.51        90\n",
            "         3.0       0.94      0.94      0.94        90\n",
            "         4.0       0.81      0.82      0.82        90\n",
            "         5.0       0.58      0.54      0.56        90\n",
            "         6.0       0.61      0.68      0.64        90\n",
            "         7.0       0.97      1.00      0.98        89\n",
            "\n",
            "    accuracy                           0.74       629\n",
            "   macro avg       0.74      0.74      0.74       629\n",
            "weighted avg       0.74      0.74      0.74       629\n",
            "\n",
            "----- Decision Tree -----\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         1.0       0.55      0.53      0.54        90\n",
            "         2.0       0.43      0.44      0.44        90\n",
            "         3.0       0.90      0.90      0.90        90\n",
            "         4.0       0.63      0.68      0.65        90\n",
            "         5.0       0.49      0.49      0.49        90\n",
            "         6.0       0.53      0.50      0.51        90\n",
            "         7.0       0.98      0.96      0.97        89\n",
            "\n",
            "    accuracy                           0.64       629\n",
            "   macro avg       0.64      0.64      0.64       629\n",
            "weighted avg       0.64      0.64      0.64       629\n",
            "\n",
            "----- Naive Bayes -----\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         1.0       0.64      0.39      0.48        90\n",
            "         2.0       0.45      0.47      0.46        90\n",
            "         3.0       0.87      0.88      0.87        90\n",
            "         4.0       0.62      0.79      0.69        90\n",
            "         5.0       0.54      0.57      0.55        90\n",
            "         6.0       0.49      0.49      0.49        90\n",
            "         7.0       0.97      0.99      0.98        89\n",
            "\n",
            "    accuracy                           0.65       629\n",
            "   macro avg       0.65      0.65      0.65       629\n",
            "weighted avg       0.65      0.65      0.65       629\n",
            "\n",
            "----- Gradient Boosting -----\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\jacob\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "c:\\Users\\jacob\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "c:\\Users\\jacob\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "c:\\Users\\jacob\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "c:\\Users\\jacob\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         1.0       0.69      0.67      0.68        90\n",
            "         2.0       0.49      0.49      0.49        90\n",
            "         3.0       0.94      0.93      0.94        90\n",
            "         4.0       0.81      0.76      0.78        90\n",
            "         5.0       0.60      0.64      0.62        90\n",
            "         6.0       0.50      0.53      0.52        90\n",
            "         7.0       0.98      0.96      0.97        89\n",
            "\n",
            "    accuracy                           0.71       629\n",
            "   macro avg       0.72      0.71      0.71       629\n",
            "weighted avg       0.72      0.71      0.71       629\n",
            "\n",
            "----- AdaBoost -----\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\jacob\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jacob\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jacob\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jacob\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         1.0       0.21      0.39      0.28        90\n",
            "         2.0       0.30      0.08      0.12        90\n",
            "         3.0       0.94      0.74      0.83        90\n",
            "         4.0       0.64      0.42      0.51        90\n",
            "         5.0       0.33      0.73      0.45        90\n",
            "         6.0       0.46      0.57      0.50        90\n",
            "         7.0       0.00      0.00      0.00        89\n",
            "\n",
            "    accuracy                           0.42       629\n",
            "   macro avg       0.41      0.42      0.39       629\n",
            "weighted avg       0.41      0.42      0.39       629\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\jacob\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jacob\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\jacob\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\jacob\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Cargar el conjunto de datos\n",
        "data = np.loadtxt('M_5.txt', delimiter='\\t', usecols=[0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11])\n",
        "x = data[:, 1:]\n",
        "y = data[:, 0]\n",
        "\n",
        "# Lista de modelos a evaluar\n",
        "models = {\n",
        "    'SVC': SVC(kernel='linear'),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'KNN': KNeighborsClassifier(),\n",
        "    'Logistic Regression': LogisticRegression(),\n",
        "    'Decision Tree': DecisionTreeClassifier(),\n",
        "    'Naive Bayes': GaussianNB(),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(),\n",
        "    'AdaBoost': AdaBoostClassifier()\n",
        "}\n",
        "\n",
        "# Validacion cruzada y evaluacion de modelos\n",
        "for model_name, model in models.items():\n",
        "    print(f\"----- {model_name} -----\")\n",
        "    \n",
        "    kf = StratifiedKFold(n_splits=5, shuffle=True)\n",
        "    \n",
        "    cv_y_test = []\n",
        "    cv_y_pred = []\n",
        "    \n",
        "    for train_index, test_index in kf.split(x, y):\n",
        "        # Fase de entrenamiento\n",
        "        x_train, y_train = x[train_index], y[train_index]\n",
        "        \n",
        "        # Entrenar el modelo\n",
        "        model.fit(x_train, y_train)\n",
        "        \n",
        "        # Fase de prueba\n",
        "        x_test, y_test = x[test_index], y[test_index]\n",
        "        y_pred = model.predict(x_test)\n",
        "        \n",
        "        cv_y_test.append(y_test)\n",
        "        cv_y_pred.append(y_pred)\n",
        "    \n",
        "    # Imprimir el reporte de clasificacion\n",
        "    print(classification_report(np.concatenate(cv_y_test), np.concatenate(cv_y_pred)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Escoge al menos dos clasificadores que hayas evaluado en el paso anterior e identifica sus hiperparámetros. Lleva a cabo el proceso de validación cruzada anidada para evaluar los dos modelos con la selección óptima de hiperparámetros.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----- KNN classifier - K parameter -----\n",
            "---- k = 1\n",
            "ACC: 0.6853206349206349\n",
            "---- k = 2\n",
            "ACC: 0.6566730158730157\n",
            "---- k = 3\n",
            "ACC: 0.7075555555555557\n",
            "---- k = 4\n",
            "ACC: 0.710615873015873\n",
            "---- k = 5\n",
            "ACC: 0.7123301587301587\n",
            "---- k = 6\n",
            "ACC: 0.7090285714285715\n",
            "---- k = 7\n",
            "ACC: 0.7170666666666667\n",
            "---- k = 8\n",
            "ACC: 0.7138412698412699\n",
            "---- k = 9\n",
            "ACC: 0.7186412698412699\n",
            "---- k = 10\n",
            "ACC: 0.7233777777777778\n",
            "---- k = 11\n",
            "ACC: 0.7138539682539683\n",
            "---- k = 12\n",
            "ACC: 0.7313396825396825\n",
            "---- k = 13\n",
            "ACC: 0.732952380952381\n",
            "---- k = 14\n",
            "ACC: 0.7234158730158731\n",
            "---- k = 15\n",
            "ACC: 0.7138920634920634\n",
            "---- k = 16\n",
            "ACC: 0.7202158730158731\n",
            "---- k = 17\n",
            "ACC: 0.7107047619047618\n",
            "---- k = 18\n",
            "ACC: 0.7154539682539683\n",
            "---- k = 19\n",
            "ACC: 0.7106920634920634\n",
            "---- k = 20\n",
            "ACC: 0.7043174603174602\n",
            "---- k = 21\n",
            "ACC: 0.7027301587301586\n",
            "---- k = 22\n",
            "ACC: 0.7090920634920634\n",
            "---- k = 23\n",
            "ACC: 0.7107047619047618\n",
            "---- k = 24\n",
            "ACC: 0.7059174603174603\n",
            "---- k = 25\n",
            "ACC: 0.7170539682539683\n",
            "---- k = 26\n",
            "ACC: 0.7154539682539683\n",
            "---- k = 27\n",
            "ACC: 0.7154666666666667\n",
            "---- k = 28\n",
            "ACC: 0.7218285714285715\n",
            "---- k = 29\n",
            "ACC: 0.7202412698412697\n",
            "---- k = 30\n",
            "ACC: 0.7186539682539681\n",
            "---- k = 31\n",
            "ACC: 0.7107047619047618\n",
            "---- k = 32\n",
            "ACC: 0.7154666666666666\n",
            "---- k = 33\n",
            "ACC: 0.7091174603174603\n",
            "---- k = 34\n",
            "ACC: 0.7107047619047618\n",
            "---- k = 35\n",
            "ACC: 0.7091301587301586\n",
            "---- k = 36\n",
            "ACC: 0.7170539682539683\n",
            "---- k = 37\n",
            "ACC: 0.7075174603174602\n",
            "---- k = 38\n",
            "ACC: 0.7123047619047618\n",
            "---- k = 39\n",
            "ACC: 0.7043555555555555\n",
            "---- k = 40\n",
            "ACC: 0.6995936507936508\n",
            "---- k = 41\n",
            "ACC: 0.7011936507936507\n",
            "---- k = 42\n",
            "ACC: 0.7027809523809523\n",
            "---- k = 43\n",
            "ACC: 0.6948190476190476\n",
            "---- k = 44\n",
            "ACC: 0.6980063492063492\n",
            "---- k = 45\n",
            "ACC: 0.7059555555555554\n",
            "---- k = 46\n",
            "ACC: 0.7091428571428571\n",
            "---- k = 47\n",
            "ACC: 0.7139301587301586\n",
            "---- k = 48\n",
            "ACC: 0.7059555555555554\n",
            "---- k = 49\n",
            "ACC: 0.7075555555555555\n",
            "---- k = 50\n",
            "ACC: 0.7059682539682539\n",
            "---- k = 51\n",
            "ACC: 0.7043555555555555\n",
            "---- k = 52\n",
            "ACC: 0.7091428571428571\n",
            "---- k = 53\n",
            "ACC: 0.7139174603174603\n",
            "---- k = 54\n",
            "ACC: 0.7107428571428571\n",
            "---- k = 55\n",
            "ACC: 0.7043555555555555\n",
            "---- k = 56\n",
            "ACC: 0.7091555555555555\n",
            "---- k = 57\n",
            "ACC: 0.7091174603174603\n",
            "---- k = 58\n",
            "ACC: 0.7091301587301586\n",
            "---- k = 59\n",
            "ACC: 0.7075555555555555\n",
            "---- k = 60\n",
            "ACC: 0.7091301587301586\n",
            "---- k = 61\n",
            "ACC: 0.7123174603174605\n",
            "---- k = 62\n",
            "ACC: 0.712279365079365\n",
            "---- k = 63\n",
            "ACC: 0.712279365079365\n",
            "---- k = 64\n",
            "ACC: 0.7106920634920634\n",
            "---- k = 65\n",
            "ACC: 0.7075174603174602\n",
            "---- k = 66\n",
            "ACC: 0.7059555555555554\n",
            "---- k = 67\n",
            "ACC: 0.7027936507936507\n",
            "---- k = 68\n",
            "ACC: 0.6964444444444444\n",
            "---- k = 69\n",
            "ACC: 0.7011809523809523\n",
            "---- k = 70\n",
            "ACC: 0.6996063492063491\n",
            "---- k = 71\n",
            "ACC: 0.6964190476190476\n",
            "---- k = 72\n",
            "ACC: 0.7027809523809523\n",
            "---- k = 73\n",
            "ACC: 0.6996063492063491\n",
            "---- k = 74\n",
            "ACC: 0.6964190476190476\n",
            "---- k = 75\n",
            "ACC: 0.7011936507936507\n",
            "---- k = 76\n",
            "ACC: 0.6980190476190475\n",
            "---- k = 77\n",
            "ACC: 0.7011682539682539\n",
            "---- k = 78\n",
            "ACC: 0.7028063492063492\n",
            "---- k = 79\n",
            "ACC: 0.7027936507936507\n",
            "---- k = 80\n",
            "ACC: 0.691631746031746\n",
            "---- k = 81\n",
            "ACC: 0.6995809523809523\n",
            "---- k = 82\n",
            "ACC: 0.6963809523809523\n",
            "---- k = 83\n",
            "ACC: 0.6900444444444445\n",
            "---- k = 84\n",
            "ACC: 0.6900444444444445\n",
            "---- k = 85\n",
            "ACC: 0.6980063492063492\n",
            "---- k = 86\n",
            "ACC: 0.696431746031746\n",
            "---- k = 87\n",
            "ACC: 0.6932444444444444\n",
            "---- k = 88\n",
            "ACC: 0.6948444444444444\n",
            "---- k = 89\n",
            "ACC: 0.6852952380952381\n",
            "---- k = 90\n",
            "ACC: 0.6837079365079365\n",
            "---- k = 91\n",
            "ACC: 0.6884825396825397\n",
            "---- k = 92\n",
            "ACC: 0.6789333333333334\n",
            "---- k = 93\n",
            "ACC: 0.6789460317460317\n",
            "---- k = 94\n",
            "ACC: 0.6789460317460317\n",
            "---- k = 95\n",
            "ACC: 0.6773587301587302\n",
            "---- k = 96\n",
            "ACC: 0.6869079365079365\n",
            "---- k = 97\n",
            "ACC: 0.6789587301587302\n",
            "---- k = 98\n",
            "ACC: 0.6868952380952381\n",
            "---- k = 99\n",
            "ACC: 0.6789333333333334\n",
            "---- k = 100\n",
            "ACC: 0.680520634920635\n",
            "---- k = 101\n",
            "ACC: 0.6932444444444444\n",
            "---- k = 102\n",
            "ACC: 0.6868698412698413\n",
            "---- k = 103\n",
            "ACC: 0.6837079365079365\n",
            "---- k = 104\n",
            "ACC: 0.6884698412698412\n",
            "---- k = 105\n",
            "ACC: 0.6820952380952381\n",
            "---- k = 106\n",
            "ACC: 0.6820952380952381\n",
            "---- k = 107\n",
            "ACC: 0.6821079365079366\n",
            "---- k = 108\n",
            "ACC: 0.6837079365079365\n",
            "---- k = 109\n",
            "ACC: 0.6837079365079365\n",
            "---- k = 110\n",
            "ACC: 0.6852952380952381\n",
            "---- k = 111\n",
            "ACC: 0.6837079365079365\n",
            "---- k = 112\n",
            "ACC: 0.6821079365079366\n",
            "---- k = 113\n",
            "ACC: 0.6836825396825397\n",
            "---- k = 114\n",
            "ACC: 0.6789079365079365\n",
            "---- k = 115\n",
            "ACC: 0.6725587301587301\n",
            "---- k = 116\n",
            "ACC: 0.6805079365079365\n",
            "---- k = 117\n",
            "ACC: 0.6788952380952381\n",
            "---- k = 118\n",
            "ACC: 0.6772825396825397\n",
            "---- k = 119\n",
            "ACC: 0.6820571428571428\n",
            "---- k = 120\n",
            "ACC: 0.6756952380952381\n",
            "---- k = 121\n",
            "ACC: 0.6804698412698412\n",
            "---- k = 122\n",
            "ACC: 0.6788698412698413\n",
            "---- k = 123\n",
            "ACC: 0.6772825396825397\n",
            "---- k = 124\n",
            "ACC: 0.6756952380952381\n",
            "---- k = 125\n",
            "ACC: 0.6756952380952381\n",
            "---- k = 126\n",
            "ACC: 0.6852571428571428\n",
            "---- k = 127\n",
            "ACC: 0.680495238095238\n",
            "---- k = 128\n",
            "ACC: 0.6757079365079365\n",
            "---- k = 129\n",
            "ACC: 0.670920634920635\n",
            "---- k = 130\n",
            "ACC: 0.6725333333333333\n",
            "---- k = 131\n",
            "ACC: 0.6645587301587301\n",
            "---- k = 132\n",
            "ACC: 0.6597968253968255\n",
            "---- k = 133\n",
            "ACC: 0.6597968253968254\n",
            "---- k = 134\n",
            "ACC: 0.6597968253968254\n",
            "---- k = 135\n",
            "ACC: 0.6597968253968254\n",
            "---- k = 136\n",
            "ACC: 0.659784126984127\n",
            "---- k = 137\n",
            "ACC: 0.6598095238095238\n",
            "---- k = 138\n",
            "ACC: 0.6581968253968254\n",
            "---- k = 139\n",
            "ACC: 0.659784126984127\n",
            "---- k = 140\n",
            "ACC: 0.6661460317460318\n",
            "---- k = 141\n",
            "ACC: 0.6693333333333334\n",
            "---- k = 142\n",
            "ACC: 0.6693460317460318\n",
            "---- k = 143\n",
            "ACC: 0.6661460317460318\n",
            "---- k = 144\n",
            "ACC: 0.6629714285714287\n",
            "---- k = 145\n",
            "ACC: 0.6645460317460319\n",
            "---- k = 146\n",
            "ACC: 0.6613714285714287\n",
            "---- k = 147\n",
            "ACC: 0.664584126984127\n",
            "---- k = 148\n",
            "ACC: 0.6645714285714287\n",
            "---- k = 149\n",
            "ACC: 0.6581968253968256\n",
            "---- k = 150\n",
            "ACC: 0.6629714285714287\n",
            "---- k = 151\n",
            "ACC: 0.6597968253968255\n",
            "---- k = 152\n",
            "ACC: 0.6550349206349207\n",
            "---- k = 153\n",
            "ACC: 0.6597968253968255\n",
            "---- k = 154\n",
            "ACC: 0.6614222222222222\n",
            "---- k = 155\n",
            "ACC: 0.6645968253968254\n",
            "---- k = 156\n",
            "ACC: 0.6614222222222222\n",
            "---- k = 157\n",
            "ACC: 0.6582476190476191\n",
            "---- k = 158\n",
            "ACC: 0.658247619047619\n",
            "---- k = 159\n",
            "ACC: 0.6486730158730158\n",
            "---- k = 160\n",
            "ACC: 0.655047619047619\n",
            "---- k = 161\n",
            "ACC: 0.6582349206349207\n",
            "---- k = 162\n",
            "ACC: 0.6550730158730158\n",
            "---- k = 163\n",
            "ACC: 0.6534857142857142\n",
            "---- k = 164\n",
            "ACC: 0.6534730158730158\n",
            "---- k = 165\n",
            "ACC: 0.6534603174603175\n",
            "---- k = 166\n",
            "ACC: 0.6534603174603174\n",
            "---- k = 167\n",
            "ACC: 0.6566603174603174\n",
            "---- k = 168\n",
            "ACC: 0.6518984126984126\n",
            "---- k = 169\n",
            "ACC: 0.6518730158730158\n",
            "---- k = 170\n",
            "ACC: 0.6582222222222222\n",
            "---- k = 171\n",
            "ACC: 0.6518603174603175\n",
            "---- k = 172\n",
            "ACC: 0.6502603174603174\n",
            "---- k = 173\n",
            "ACC: 0.6566349206349206\n",
            "---- k = 174\n",
            "ACC: 0.6550349206349206\n",
            "---- k = 175\n",
            "ACC: 0.6502984126984126\n",
            "---- k = 176\n",
            "ACC: 0.6503111111111111\n",
            "---- k = 177\n",
            "ACC: 0.6534984126984126\n",
            "---- k = 178\n",
            "ACC: 0.6487365079365078\n",
            "---- k = 179\n",
            "ACC: 0.6534857142857142\n",
            "---- k = 180\n",
            "ACC: 0.6550857142857142\n",
            "---- k = 181\n",
            "ACC: 0.6471238095238094\n",
            "---- k = 182\n",
            "ACC: 0.648711111111111\n",
            "---- k = 183\n",
            "ACC: 0.6518857142857142\n",
            "---- k = 184\n",
            "ACC: 0.6550476190476191\n",
            "---- k = 185\n",
            "ACC: 0.6518857142857142\n",
            "---- k = 186\n",
            "ACC: 0.6471238095238094\n",
            "---- k = 187\n",
            "ACC: 0.6518857142857142\n",
            "---- k = 188\n",
            "ACC: 0.6487238095238095\n",
            "---- k = 189\n",
            "ACC: 0.6503238095238094\n",
            "---- k = 190\n",
            "ACC: 0.6471238095238094\n",
            "---- k = 191\n",
            "ACC: 0.648711111111111\n",
            "---- k = 192\n",
            "ACC: 0.6518857142857142\n",
            "---- k = 193\n",
            "ACC: 0.6487238095238095\n",
            "---- k = 194\n",
            "ACC: 0.648711111111111\n",
            "---- k = 195\n",
            "ACC: 0.6471238095238094\n",
            "---- k = 196\n",
            "ACC: 0.6455238095238095\n",
            "---- k = 197\n",
            "ACC: 0.6439238095238096\n",
            "---- k = 198\n",
            "ACC: 0.6455111111111111\n",
            "---- k = 199\n",
            "ACC: 0.6439111111111111\n",
            "---- k = 200\n",
            "ACC: 0.6486857142857143\n",
            "---- k = 201\n",
            "ACC: 0.6471238095238094\n",
            "---- k = 202\n",
            "ACC: 0.6471238095238094\n",
            "---- k = 203\n",
            "ACC: 0.6471111111111111\n",
            "---- k = 204\n",
            "ACC: 0.6439365079365079\n",
            "---- k = 205\n",
            "ACC: 0.6439365079365079\n",
            "---- k = 206\n",
            "ACC: 0.6439365079365079\n",
            "---- k = 207\n",
            "ACC: 0.6375873015873016\n",
            "---- k = 208\n",
            "ACC: 0.6391873015873015\n",
            "---- k = 209\n",
            "ACC: 0.6376\n",
            "---- k = 210\n",
            "ACC: 0.6407746031746031\n",
            "---- k = 211\n",
            "ACC: 0.6407746031746031\n",
            "---- k = 212\n",
            "ACC: 0.636\n",
            "---- k = 213\n",
            "ACC: 0.6344000000000001\n",
            "---- k = 214\n",
            "ACC: 0.6248761904761906\n",
            "---- k = 215\n",
            "ACC: 0.6248761904761906\n",
            "---- k = 216\n",
            "ACC: 0.6201142857142858\n",
            "---- k = 217\n",
            "ACC: 0.623288888888889\n",
            "---- k = 218\n",
            "ACC: 0.6280507936507937\n",
            "---- k = 219\n",
            "ACC: 0.6296380952380953\n",
            "---- k = 220\n",
            "ACC: 0.6280507936507937\n",
            "---- k = 221\n",
            "ACC: 0.6217015873015873\n",
            "---- k = 222\n",
            "ACC: 0.6233015873015874\n",
            "---- k = 223\n",
            "ACC: 0.6249015873015873\n",
            "---- k = 224\n",
            "ACC: 0.6233015873015874\n",
            "---- k = 225\n",
            "ACC: 0.6201142857142857\n",
            "---- k = 226\n",
            "ACC: 0.6185269841269841\n",
            "---- k = 227\n",
            "ACC: 0.6233015873015874\n",
            "---- k = 228\n",
            "ACC: 0.6248888888888889\n",
            "---- k = 229\n",
            "ACC: 0.6217142857142857\n",
            "---- k = 230\n",
            "ACC: 0.6201269841269841\n",
            "---- k = 231\n",
            "ACC: 0.6185269841269841\n",
            "---- k = 232\n",
            "ACC: 0.618552380952381\n",
            "---- k = 233\n",
            "ACC: 0.6153904761904763\n",
            "---- k = 234\n",
            "ACC: 0.620152380952381\n",
            "---- k = 235\n",
            "ACC: 0.618552380952381\n",
            "---- k = 236\n",
            "ACC: 0.6217269841269841\n",
            "---- k = 237\n",
            "ACC: 0.6217142857142857\n",
            "---- k = 238\n",
            "ACC: 0.615352380952381\n",
            "---- k = 239\n",
            "ACC: 0.6073904761904763\n",
            "---- k = 240\n",
            "ACC: 0.6105777777777778\n",
            "---- k = 241\n",
            "ACC: 0.6121650793650794\n",
            "---- k = 242\n",
            "ACC: 0.6137396825396826\n",
            "---- k = 243\n",
            "ACC: 0.6105650793650794\n",
            "---- k = 244\n",
            "ACC: 0.6058031746031747\n",
            "---- k = 245\n",
            "ACC: 0.6042158730158731\n",
            "---- k = 246\n",
            "ACC: 0.5994412698412699\n",
            "---- k = 247\n",
            "ACC: 0.5962793650793652\n",
            "---- k = 248\n",
            "ACC: 0.5962793650793652\n",
            "---- k = 249\n",
            "ACC: 0.6010539682539683\n",
            "---- k = 250\n",
            "ACC: 0.5931174603174604\n",
            "---- k = 251\n",
            "ACC: 0.5962920634920635\n",
            "---- k = 252\n",
            "ACC: 0.5962666666666667\n",
            "---- k = 253\n",
            "ACC: 0.6010285714285715\n",
            "---- k = 254\n",
            "ACC: 0.602615873015873\n",
            "---- k = 255\n",
            "ACC: 0.6073904761904763\n",
            "---- k = 256\n",
            "ACC: 0.601015873015873\n",
            "---- k = 257\n",
            "ACC: 0.5994285714285714\n",
            "---- k = 258\n",
            "ACC: 0.5978539682539683\n",
            "---- k = 259\n",
            "ACC: 0.6010285714285715\n",
            "---- k = 260\n",
            "ACC: 0.5994412698412699\n",
            "---- k = 261\n",
            "ACC: 0.6105650793650794\n",
            "---- k = 262\n",
            "ACC: 0.610552380952381\n",
            "---- k = 263\n",
            "ACC: 0.6089650793650794\n",
            "---- k = 264\n",
            "ACC: 0.6089650793650794\n",
            "---- k = 265\n",
            "ACC: 0.6026285714285715\n",
            "---- k = 266\n",
            "ACC: 0.6026412698412699\n",
            "---- k = 267\n",
            "ACC: 0.6026412698412699\n",
            "---- k = 268\n",
            "ACC: 0.5994539682539684\n",
            "---- k = 269\n",
            "ACC: 0.6010539682539683\n",
            "---- k = 270\n",
            "ACC: 0.6010539682539683\n",
            "---- k = 271\n",
            "ACC: 0.5994539682539684\n",
            "---- k = 272\n",
            "ACC: 0.5946666666666667\n",
            "---- k = 273\n",
            "ACC: 0.5930920634920636\n",
            "---- k = 274\n",
            "ACC: 0.5931047619047619\n",
            "---- k = 275\n",
            "ACC: 0.5931174603174603\n",
            "---- k = 276\n",
            "ACC: 0.5899428571428571\n",
            "---- k = 277\n",
            "ACC: 0.5899428571428571\n",
            "---- k = 278\n",
            "ACC: 0.5931174603174603\n",
            "---- k = 279\n",
            "ACC: 0.5947174603174603\n",
            "---- k = 280\n",
            "ACC: 0.5947047619047618\n",
            "---- k = 281\n",
            "ACC: 0.5931047619047619\n",
            "---- k = 282\n",
            "ACC: 0.5915301587301587\n",
            "---- k = 283\n",
            "ACC: 0.5963047619047619\n",
            "---- k = 284\n",
            "ACC: 0.5978920634920635\n",
            "---- k = 285\n",
            "ACC: 0.6010666666666667\n",
            "---- k = 286\n",
            "ACC: 0.6026793650793649\n",
            "---- k = 287\n",
            "ACC: 0.6010666666666666\n",
            "---- k = 288\n",
            "ACC: 0.5962666666666666\n",
            "---- k = 289\n",
            "ACC: 0.5962539682539683\n",
            "---- k = 290\n",
            "ACC: 0.5994285714285714\n",
            "---- k = 291\n",
            "ACC: 0.5914539682539683\n",
            "---- k = 292\n",
            "ACC: 0.5914666666666667\n",
            "---- k = 293\n",
            "ACC: 0.5962539682539683\n",
            "---- k = 294\n",
            "ACC: 0.5930666666666667\n",
            "---- k = 295\n",
            "ACC: 0.5946666666666667\n",
            "---- k = 296\n",
            "ACC: 0.5930793650793651\n",
            "---- k = 297\n",
            "ACC: 0.5899301587301587\n",
            "---- k = 298\n",
            "ACC: 0.5915174603174603\n",
            "---- k = 299\n",
            "ACC: 0.5899301587301587\n",
            "---- k = 300\n",
            "ACC: 0.5835555555555556\n",
            "---- k = 301\n",
            "ACC: 0.581968253968254\n",
            "---- k = 302\n",
            "ACC: 0.5756063492063492\n",
            "---- k = 303\n",
            "ACC: 0.5724190476190476\n",
            "---- k = 304\n",
            "ACC: 0.570831746031746\n",
            "---- k = 305\n",
            "ACC: 0.5708444444444445\n",
            "---- k = 306\n",
            "ACC: 0.5676444444444445\n",
            "---- k = 307\n",
            "ACC: 0.5644698412698413\n",
            "---- k = 308\n",
            "ACC: 0.5660571428571429\n",
            "---- k = 309\n",
            "ACC: 0.5660571428571429\n",
            "---- k = 310\n",
            "ACC: 0.5660571428571429\n",
            "---- k = 311\n",
            "ACC: 0.5660571428571429\n",
            "---- k = 312\n",
            "ACC: 0.5580825396825397\n",
            "---- k = 313\n",
            "ACC: 0.5533079365079365\n",
            "---- k = 314\n",
            "ACC: 0.5548952380952381\n",
            "---- k = 315\n",
            "ACC: 0.5564952380952382\n",
            "---- k = 316\n",
            "ACC: 0.5596952380952381\n",
            "---- k = 317\n",
            "ACC: 0.5596952380952381\n",
            "---- k = 318\n",
            "ACC: 0.5580952380952382\n",
            "---- k = 319\n",
            "ACC: 0.5564952380952382\n",
            "---- k = 320\n",
            "ACC: 0.551720634920635\n",
            "---- k = 321\n",
            "ACC: 0.5485333333333333\n",
            "---- k = 322\n",
            "ACC: 0.5485206349206349\n",
            "---- k = 323\n",
            "ACC: 0.5485206349206349\n",
            "---- k = 324\n",
            "ACC: 0.5469333333333333\n",
            "---- k = 325\n",
            "ACC: 0.5469333333333334\n",
            "---- k = 326\n",
            "ACC: 0.550120634920635\n",
            "---- k = 327\n",
            "ACC: 0.5501333333333334\n",
            "---- k = 328\n",
            "ACC: 0.5485460317460318\n",
            "---- k = 329\n",
            "ACC: 0.5469460317460318\n",
            "---- k = 330\n",
            "ACC: 0.5453587301587303\n",
            "---- k = 331\n",
            "ACC: 0.540584126984127\n",
            "---- k = 332\n",
            "ACC: 0.5373968253968254\n",
            "---- k = 333\n",
            "ACC: 0.5358222222222222\n",
            "---- k = 334\n",
            "ACC: 0.532647619047619\n",
            "---- k = 335\n",
            "ACC: 0.5262984126984127\n",
            "---- k = 336\n",
            "ACC: 0.5262984126984127\n",
            "---- k = 337\n",
            "ACC: 0.5262984126984127\n",
            "---- k = 338\n",
            "ACC: 0.5263111111111111\n",
            "---- k = 339\n",
            "ACC: 0.5231365079365079\n",
            "---- k = 340\n",
            "ACC: 0.5151873015873016\n",
            "---- k = 341\n",
            "ACC: 0.5135873015873016\n",
            "---- k = 342\n",
            "ACC: 0.512\n",
            "---- k = 343\n",
            "ACC: 0.5135873015873016\n",
            "---- k = 344\n",
            "ACC: 0.512\n",
            "---- k = 345\n",
            "ACC: 0.5104253968253969\n",
            "---- k = 346\n",
            "ACC: 0.5088507936507937\n",
            "---- k = 347\n",
            "ACC: 0.5056761904761905\n",
            "---- k = 348\n",
            "ACC: 0.4993142857142857\n",
            "---- k = 349\n",
            "ACC: 0.49771428571428566\n",
            "---- k = 350\n",
            "ACC: 0.4945396825396825\n",
            "---- k = 351\n",
            "ACC: 0.4881904761904762\n",
            "---- k = 352\n",
            "ACC: 0.48026666666666673\n",
            "---- k = 353\n",
            "ACC: 0.4707301587301588\n",
            "---- k = 354\n",
            "ACC: 0.46755555555555556\n",
            "---- k = 355\n",
            "ACC: 0.46120634920634923\n",
            "---- k = 356\n",
            "ACC: 0.45009523809523805\n",
            "---- k = 357\n",
            "ACC: 0.44057142857142856\n",
            "---- k = 358\n",
            "ACC: 0.43103492063492066\n",
            "---- k = 359\n",
            "ACC: 0.43262222222222224\n",
            "---- k = 360\n",
            "ACC: 0.4278603174603175\n",
            "---- k = 361\n",
            "ACC: 0.4262603174603175\n",
            "---- k = 362\n",
            "ACC: 0.42147301587301583\n",
            "---- k = 363\n",
            "ACC: 0.42147301587301583\n",
            "---- k = 364\n",
            "ACC: 0.4151111111111111\n",
            "---- k = 365\n",
            "ACC: 0.4135238095238095\n",
            "---- k = 366\n",
            "ACC: 0.4087492063492063\n",
            "---- k = 367\n",
            "ACC: 0.4087492063492063\n",
            "---- k = 368\n",
            "ACC: 0.40556190476190473\n",
            "---- k = 369\n",
            "ACC: 0.3991873015873016\n",
            "---- k = 370\n",
            "ACC: 0.3960126984126984\n",
            "---- k = 371\n",
            "ACC: 0.38008888888888887\n",
            "---- k = 372\n",
            "ACC: 0.3753142857142857\n",
            "---- k = 373\n",
            "ACC: 0.3753015873015873\n",
            "---- k = 374\n",
            "ACC: 0.3673269841269841\n",
            "---- k = 375\n",
            "ACC: 0.3609523809523809\n",
            "---- k = 376\n",
            "ACC: 0.3593396825396825\n",
            "---- k = 377\n",
            "ACC: 0.34820317460317457\n",
            "---- k = 378\n",
            "ACC: 0.3434285714285714\n",
            "---- k = 379\n",
            "ACC: 0.341815873015873\n",
            "---- k = 380\n",
            "ACC: 0.3370539682539683\n",
            "---- k = 381\n",
            "ACC: 0.3354666666666667\n",
            "---- k = 382\n",
            "ACC: 0.3274920634920635\n",
            "---- k = 383\n",
            "ACC: 0.32431746031746034\n",
            "---- k = 384\n",
            "ACC: 0.3211428571428572\n",
            "---- k = 385\n",
            "ACC: 0.3147809523809524\n",
            "---- k = 386\n",
            "ACC: 0.31000634920634923\n",
            "---- k = 387\n",
            "ACC: 0.30365714285714285\n",
            "---- k = 388\n",
            "ACC: 0.3020825396825397\n",
            "---- k = 389\n",
            "ACC: 0.28775873015873016\n",
            "---- k = 390\n",
            "ACC: 0.28297142857142854\n",
            "---- k = 391\n",
            "ACC: 0.27343492063492064\n",
            "---- k = 392\n",
            "ACC: 0.2607238095238095\n",
            "---- k = 393\n",
            "ACC: 0.25436190476190473\n",
            "---- k = 394\n",
            "ACC: 0.24482539682539678\n",
            "---- k = 395\n",
            "ACC: 0.24005079365079363\n",
            "---- k = 396\n",
            "ACC: 0.23528888888888888\n",
            "---- k = 397\n",
            "ACC: 0.22735238095238097\n",
            "---- k = 398\n",
            "ACC: 0.22576507936507934\n",
            "---- k = 399\n",
            "ACC: 0.2193904761904762\n",
            "---- k = 400\n",
            "ACC: 0.21302857142857143\n",
            "---- k = 401\n",
            "ACC: 0.21305396825396822\n",
            "---- k = 402\n",
            "ACC: 0.20829206349206347\n",
            "---- k = 403\n",
            "ACC: 0.20194285714285715\n",
            "---- k = 404\n",
            "ACC: 0.19876825396825398\n",
            "---- k = 405\n",
            "ACC: 0.19558095238095236\n",
            "---- k = 406\n",
            "ACC: 0.19239365079365078\n",
            "---- k = 407\n",
            "ACC: 0.18921904761904762\n",
            "---- k = 408\n",
            "ACC: 0.1892063492063492\n",
            "---- k = 409\n",
            "ACC: 0.18604444444444443\n",
            "---- k = 410\n",
            "ACC: 0.1860444444444444\n",
            "---- k = 411\n",
            "ACC: 0.18445714285714282\n",
            "---- k = 412\n",
            "ACC: 0.18605714285714284\n",
            "---- k = 413\n",
            "ACC: 0.1812952380952381\n",
            "---- k = 414\n",
            "ACC: 0.1797079365079365\n",
            "---- k = 415\n",
            "ACC: 0.18128253968253966\n",
            "---- k = 416\n",
            "ACC: 0.1812952380952381\n",
            "---- k = 417\n",
            "ACC: 0.1812952380952381\n",
            "---- k = 418\n",
            "ACC: 0.1812952380952381\n",
            "---- k = 419\n",
            "ACC: 0.18286984126984124\n",
            "---- k = 420\n",
            "ACC: 0.18128253968253966\n",
            "---- k = 421\n",
            "ACC: 0.17488253968253967\n",
            "---- k = 422\n",
            "ACC: 0.17012063492063492\n",
            "---- k = 423\n",
            "ACC: 0.16535873015873012\n",
            "---- k = 424\n",
            "ACC: 0.16693333333333332\n",
            "---- k = 425\n",
            "ACC: 0.16218412698412696\n",
            "---- k = 426\n",
            "ACC: 0.15746031746031744\n",
            "---- k = 427\n",
            "ACC: 0.16224761904761903\n",
            "---- k = 428\n",
            "ACC: 0.18608253968253968\n",
            "---- k = 429\n",
            "ACC: 0.18606984126984125\n",
            "---- k = 430\n",
            "ACC: 0.18288253968253967\n",
            "---- k = 431\n",
            "ACC: 0.18286984126984124\n",
            "---- k = 432\n",
            "ACC: 0.18286984126984124\n",
            "---- k = 433\n",
            "ACC: 0.18921904761904756\n",
            "---- k = 434\n",
            "ACC: 0.18763174603174598\n",
            "---- k = 435\n",
            "ACC: 0.18763174603174598\n",
            "---- k = 436\n",
            "ACC: 0.18763174603174598\n",
            "---- k = 437\n",
            "ACC: 0.18763174603174598\n",
            "---- k = 438\n",
            "ACC: 0.18763174603174598\n",
            "---- k = 439\n",
            "ACC: 0.1860444444444444\n",
            "---- k = 440\n",
            "ACC: 0.1844444444444444\n",
            "---- k = 441\n",
            "ACC: 0.18284444444444442\n",
            "---- k = 442\n",
            "ACC: 0.186031746031746\n",
            "---- k = 443\n",
            "ACC: 0.18285714285714283\n",
            "---- k = 444\n",
            "ACC: 0.18285714285714283\n",
            "---- k = 445\n",
            "ACC: 0.18284444444444442\n",
            "---- k = 446\n",
            "ACC: 0.18125714285714284\n",
            "---- k = 447\n",
            "ACC: 0.18284444444444442\n",
            "---- k = 448\n",
            "ACC: 0.18284444444444442\n",
            "---- k = 449\n",
            "ACC: 0.18443174603174603\n",
            "---- k = 450\n",
            "ACC: 0.1860190476190476\n",
            "---- k = 451\n",
            "ACC: 0.18283174603174604\n",
            "---- k = 452\n",
            "ACC: 0.18283174603174604\n",
            "---- k = 453\n",
            "ACC: 0.18124444444444446\n",
            "---- k = 454\n",
            "ACC: 0.18124444444444446\n",
            "---- k = 455\n",
            "ACC: 0.18124444444444446\n",
            "---- k = 456\n",
            "ACC: 0.18124444444444446\n",
            "---- k = 457\n",
            "ACC: 0.18124444444444446\n",
            "---- k = 458\n",
            "ACC: 0.18124444444444446\n",
            "---- k = 459\n",
            "ACC: 0.17484444444444444\n",
            "---- k = 460\n",
            "ACC: 0.16845714285714283\n",
            "---- k = 461\n",
            "ACC: 0.17005714285714285\n",
            "---- k = 462\n",
            "ACC: 0.16368253968253968\n",
            "---- k = 463\n",
            "ACC: 0.1652952380952381\n",
            "---- k = 464\n",
            "ACC: 0.1701079365079365\n",
            "---- k = 465\n",
            "ACC: 0.1701079365079365\n",
            "---- k = 466\n",
            "ACC: 0.1701079365079365\n",
            "---- k = 467\n",
            "ACC: 0.1685206349206349\n",
            "---- k = 468\n",
            "ACC: 0.1685206349206349\n",
            "---- k = 469\n",
            "ACC: 0.1685206349206349\n",
            "---- k = 470\n",
            "ACC: 0.1669206349206349\n",
            "---- k = 471\n",
            "ACC: 0.16533333333333333\n",
            "---- k = 472\n",
            "ACC: 0.16373333333333334\n",
            "---- k = 473\n",
            "ACC: 0.16373333333333334\n",
            "---- k = 474\n",
            "ACC: 0.16373333333333334\n",
            "---- k = 475\n",
            "ACC: 0.16373333333333334\n",
            "---- k = 476\n",
            "ACC: 0.16214603174603173\n",
            "---- k = 477\n",
            "ACC: 0.16055873015873015\n",
            "---- k = 478\n",
            "ACC: 0.15897142857142857\n",
            "---- k = 479\n",
            "ACC: 0.15737142857142855\n",
            "---- k = 480\n",
            "ACC: 0.15578412698412697\n",
            "---- k = 481\n",
            "ACC: 0.1525968253968254\n",
            "---- k = 482\n",
            "ACC: 0.1525968253968254\n",
            "---- k = 483\n",
            "ACC: 0.1526095238095238\n",
            "---- k = 484\n",
            "ACC: 0.14784761904761906\n",
            "---- k = 485\n",
            "ACC: 0.13038730158730158\n",
            "---- k = 486\n",
            "ACC: 0.13514920634920635\n",
            "---- k = 487\n",
            "ACC: 0.14626031746031748\n",
            "---- k = 488\n",
            "ACC: 0.12402539682539682\n",
            "---- k = 489\n",
            "ACC: 0.13354920634920636\n",
            "---- k = 490\n",
            "ACC: 0.12561269841269843\n",
            "---- k = 491\n",
            "ACC: 0.12561269841269843\n",
            "---- k = 492\n",
            "ACC: 0.14148571428571427\n",
            "---- k = 493\n",
            "ACC: 0.14148571428571427\n",
            "---- k = 494\n",
            "ACC: 0.14148571428571427\n",
            "---- k = 495\n",
            "ACC: 0.1398984126984127\n",
            "---- k = 496\n",
            "ACC: 0.1398984126984127\n",
            "---- k = 497\n",
            "ACC: 0.14148571428571427\n",
            "---- k = 498\n",
            "ACC: 0.14148571428571427\n",
            "---- k = 499\n",
            "ACC: 0.14148571428571427\n",
            "---- k = 500\n",
            "ACC: 0.14148571428571427\n",
            "---- k = 501\n",
            "ACC: 0.1430857142857143\n",
            "---- k = 502\n",
            "ACC: 0.1430857142857143\n",
            "Optimal k:  13\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----- SVM classifier - Regularization parameter -----\n",
            "---- C = 0.001\n",
            "ACC: 0.6057904761904763\n",
            "---- C = 0.0010974987654930556\n",
            "ACC: 0.6042158730158731\n",
            "---- C = 0.0012045035402587824\n",
            "ACC: 0.618552380952381\n",
            "---- C = 0.0013219411484660286\n",
            "ACC: 0.620152380952381\n",
            "---- C = 0.0014508287784959402\n",
            "ACC: 0.6217396825396826\n",
            "---- C = 0.0015922827933410922\n",
            "ACC: 0.6249396825396826\n",
            "---- C = 0.001747528400007683\n",
            "ACC: 0.6265142857142857\n",
            "---- C = 0.0019179102616724887\n",
            "ACC: 0.6313015873015873\n",
            "---- C = 0.00210490414451202\n",
            "ACC: 0.6408380952380952\n",
            "---- C = 0.0023101297000831605\n",
            "ACC: 0.6456\n",
            "---- C = 0.0025353644939701114\n",
            "ACC: 0.6535492063492063\n",
            "---- C = 0.0027825594022071257\n",
            "ACC: 0.6519238095238095\n",
            "---- C = 0.0030538555088334154\n",
            "ACC: 0.6550730158730158\n",
            "---- C = 0.003351602650938841\n",
            "ACC: 0.6598349206349207\n",
            "---- C = 0.0036783797718286343\n",
            "ACC: 0.669384126984127\n",
            "---- C = 0.004037017258596553\n",
            "ACC: 0.6757460317460318\n",
            "---- C = 0.004430621457583882\n",
            "ACC: 0.680495238095238\n",
            "---- C = 0.004862601580065354\n",
            "ACC: 0.6789079365079365\n",
            "---- C = 0.005336699231206312\n",
            "ACC: 0.6884444444444444\n",
            "---- C = 0.005857020818056668\n",
            "ACC: 0.699568253968254\n",
            "---- C = 0.006428073117284319\n",
            "ACC: 0.7075174603174602\n",
            "---- C = 0.007054802310718645\n",
            "ACC: 0.7234412698412699\n",
            "---- C = 0.007742636826811269\n",
            "ACC: 0.7234539682539683\n",
            "---- C = 0.008497534359086447\n",
            "ACC: 0.7250412698412699\n",
            "---- C = 0.0093260334688322\n",
            "ACC: 0.7202539682539684\n",
            "---- C = 0.010235310218990263\n",
            "ACC: 0.7250285714285715\n",
            "---- C = 0.011233240329780276\n",
            "ACC: 0.7361650793650794\n",
            "---- C = 0.012328467394420665\n",
            "ACC: 0.7393269841269842\n",
            "---- C = 0.013530477745798075\n",
            "ACC: 0.7393142857142857\n",
            "---- C = 0.01484968262254465\n",
            "ACC: 0.7377142857142858\n",
            "---- C = 0.016297508346206444\n",
            "ACC: 0.7361269841269842\n",
            "---- C = 0.01788649529057435\n",
            "ACC: 0.7377015873015873\n",
            "---- C = 0.019630406500402715\n",
            "ACC: 0.7345269841269841\n",
            "---- C = 0.021544346900318846\n",
            "ACC: 0.7377015873015873\n",
            "---- C = 0.023644894126454083\n",
            "ACC: 0.7376888888888888\n",
            "---- C = 0.025950242113997372\n",
            "ACC: 0.7376888888888888\n",
            "---- C = 0.02848035868435802\n",
            "ACC: 0.7361015873015873\n",
            "---- C = 0.03125715849688237\n",
            "ACC: 0.7392761904761904\n",
            "---- C = 0.03430469286314919\n",
            "ACC: 0.7376888888888888\n",
            "---- C = 0.037649358067924694\n",
            "ACC: 0.7313396825396825\n",
            "---- C = 0.04132012400115339\n",
            "ACC: 0.731352380952381\n",
            "---- C = 0.04534878508128584\n",
            "ACC: 0.7281650793650793\n",
            "---- C = 0.049770235643321115\n",
            "ACC: 0.7233904761904763\n",
            "---- C = 0.05462277217684343\n",
            "ACC: 0.7202031746031746\n",
            "---- C = 0.05994842503189412\n",
            "ACC: 0.7185904761904762\n",
            "---- C = 0.06579332246575682\n",
            "ACC: 0.7233396825396825\n",
            "---- C = 0.07220809018385467\n",
            "ACC: 0.7233396825396825\n",
            "---- C = 0.07924828983539177\n",
            "ACC: 0.7297015873015873\n",
            "---- C = 0.08697490026177834\n",
            "ACC: 0.7297015873015873\n",
            "---- C = 0.09545484566618342\n",
            "ACC: 0.7344888888888889\n",
            "---- C = 0.10476157527896651\n",
            "ACC: 0.7345015873015873\n",
            "---- C = 0.11497569953977368\n",
            "ACC: 0.7376761904761905\n",
            "---- C = 0.1261856883066021\n",
            "ACC: 0.7440380952380952\n",
            "---- C = 0.1384886371393873\n",
            "ACC: 0.7440380952380952\n",
            "---- C = 0.15199110829529347\n",
            "ACC: 0.7440380952380952\n",
            "---- C = 0.1668100537200059\n",
            "ACC: 0.7440380952380952\n",
            "---- C = 0.18307382802953698\n",
            "ACC: 0.7440380952380952\n",
            "---- C = 0.2009233002565048\n",
            "ACC: 0.7456253968253967\n",
            "---- C = 0.22051307399030456\n",
            "ACC: 0.7440253968253968\n",
            "---- C = 0.24201282647943834\n",
            "ACC: 0.7472\n",
            "---- C = 0.26560877829466867\n",
            "ACC: 0.7488126984126984\n",
            "---- C = 0.29150530628251786\n",
            "ACC: 0.7488126984126984\n",
            "---- C = 0.31992671377973847\n",
            "ACC: 0.7487999999999999\n",
            "---- C = 0.35111917342151344\n",
            "ACC: 0.7440126984126983\n",
            "---- C = 0.3853528593710531\n",
            "ACC: 0.7455999999999999\n",
            "---- C = 0.4229242874389499\n",
            "ACC: 0.7440253968253968\n",
            "---- C = 0.4641588833612782\n",
            "ACC: 0.7424253968253968\n",
            "---- C = 0.5094138014816381\n",
            "ACC: 0.7440380952380952\n",
            "---- C = 0.5590810182512228\n",
            "ACC: 0.7424507936507936\n",
            "---- C = 0.6135907273413176\n",
            "ACC: 0.7424507936507936\n",
            "---- C = 0.6734150657750828\n",
            "ACC: 0.742463492063492\n",
            "---- C = 0.7390722033525783\n",
            "ACC: 0.7424761904761905\n",
            "---- C = 0.8111308307896873\n",
            "ACC: 0.7456507936507937\n",
            "---- C = 0.8902150854450392\n",
            "ACC: 0.7456507936507937\n",
            "---- C = 0.9770099572992257\n",
            "ACC: 0.7488253968253968\n",
            "---- C = 1.0722672220103242\n",
            "ACC: 0.7504253968253968\n",
            "---- C = 1.1768119524349991\n",
            "ACC: 0.7567746031746031\n",
            "---- C = 1.291549665014884\n",
            "ACC: 0.7552\n",
            "---- C = 1.4174741629268062\n",
            "ACC: 0.7567999999999999\n",
            "---- C = 1.5556761439304723\n",
            "ACC: 0.7584\n",
            "---- C = 1.7073526474706922\n",
            "ACC: 0.7552126984126983\n",
            "---- C = 1.873817422860385\n",
            "ACC: 0.7567873015873016\n",
            "---- C = 2.0565123083486534\n",
            "ACC: 0.7583746031746031\n",
            "---- C = 2.2570197196339215\n",
            "ACC: 0.7647238095238095\n",
            "---- C = 2.4770763559917115\n",
            "ACC: 0.7615492063492063\n",
            "---- C = 2.718588242732943\n",
            "ACC: 0.7599619047619047\n",
            "---- C = 2.9836472402833403\n",
            "ACC: 0.7583746031746031\n",
            "---- C = 3.2745491628777317\n",
            "ACC: 0.7599619047619047\n",
            "---- C = 3.5938136638046294\n",
            "ACC: 0.7567873015873016\n",
            "---- C = 3.94420605943766\n",
            "ACC: 0.7583746031746031\n",
            "---- C = 4.328761281083062\n",
            "ACC: 0.7552\n",
            "---- C = 4.750810162102798\n",
            "ACC: 0.7567873015873016\n",
            "---- C = 5.21400828799969\n",
            "ACC: 0.7583746031746031\n",
            "---- C = 5.72236765935022\n",
            "ACC: 0.7551873015873015\n",
            "---- C = 6.280291441834259\n",
            "ACC: 0.7552\n",
            "---- C = 6.892612104349702\n",
            "ACC: 0.7567873015873016\n",
            "---- C = 7.56463327554629\n",
            "ACC: 0.7583746031746031\n",
            "---- C = 8.302175681319753\n",
            "ACC: 0.7504253968253968\n",
            "---- C = 9.111627561154895\n",
            "ACC: 0.7488253968253968\n",
            "---- C = 10.0\n",
            "ACC: 0.7504126984126984\n",
            "Optimal C:  2.2570197196339215\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: black;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-1 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-1 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: block;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-1 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-1 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 1ex;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-1 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=np.float64(2.2570197196339215), kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;SVC<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.svm.SVC.html\">?<span>Documentation for SVC</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>SVC(C=np.float64(2.2570197196339215), kernel=&#x27;linear&#x27;)</pre></div> </div></div></div></div>"
            ],
            "text/plain": [
              "SVC(C=np.float64(2.2570197196339215), kernel='linear')"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Cargar el conjunto de datos\n",
        "data = np.loadtxt('M_5.txt', delimiter='\\t', usecols=[0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11])\n",
        "\n",
        "# Verificar y limpiar los datos\n",
        "x = data[:, 1:]\n",
        "y = data[:, 0]\n",
        "\n",
        "# Reemplazar NaN con valores especificos\n",
        "x = np.nan_to_num(x, nan=np.nanmean(x), posinf=np.nanmean(x), neginf=np.nanmean(x))\n",
        "\n",
        "#------------------------------------------------------------------------------------------------------------------\n",
        "# KNN classifier - Seleccion del parametro K\n",
        "#------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "print(\"----- KNN classifier - K parameter -----\")\n",
        "\n",
        "kk = np.arange(1, 503)\n",
        "acc = []\n",
        "\n",
        "for k in kk:\n",
        "    print('---- k =', k)\n",
        "    \n",
        "    acc_cv = []\n",
        "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    for train_index, test_index in kf.split(x, y):\n",
        "        # Entrenamiento\n",
        "        x_train, y_train = x[train_index], y[train_index]     \n",
        "        clf_cv = KNeighborsClassifier(n_neighbors=k)\n",
        "        clf_cv.fit(x_train, y_train)\n",
        "\n",
        "        # Prueba\n",
        "        x_test, y_test = x[test_index], y[test_index]\n",
        "        y_pred = clf_cv.predict(x_test)    \n",
        "        acc_cv.append(accuracy_score(y_test, y_pred))\n",
        "\n",
        "    acc_hyp = np.mean(acc_cv)\n",
        "    acc.append(acc_hyp)\n",
        "    print('ACC:', acc_hyp)\n",
        "\n",
        "opt_index = np.argmax(acc)\n",
        "opt_hyperparameter = kk[opt_index]\n",
        "best_k = opt_hyperparameter\n",
        "print(\"Optimal k: \", best_k)\n",
        "\n",
        "plt.plot(kk, acc)\n",
        "plt.xlabel(\"k\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"KNN - Accuracy vs. k\")\n",
        "plt.show()\n",
        "\n",
        "# Entrenar el modelo con el parametro optimo\n",
        "clf_knn = KNeighborsClassifier(n_neighbors=opt_hyperparameter)\n",
        "clf_knn.fit(x, y)\n",
        "\n",
        "#------------------------------------------------------------------------------------------------------------------\n",
        "# SVM classifier - Seleccion del parametro C\n",
        "#------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "print(\"----- SVM classifier - Regularization parameter -----\")\n",
        "\n",
        "cc = np.logspace(-3, 1, 100)\n",
        "acc = []\n",
        "\n",
        "for c in cc:\n",
        "    print('---- C =', c)\n",
        "    \n",
        "    acc_cv = []\n",
        "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    for train_index, test_index in kf.split(x, y):\n",
        "        # Entrenamiento\n",
        "        x_train, y_train = x[train_index], y[train_index]\n",
        "        clf_cv = SVC(C=c, kernel='linear')\n",
        "        clf_cv.fit(x_train, y_train)\n",
        "\n",
        "        # Prueba\n",
        "        x_test, y_test = x[test_index], y[test_index]\n",
        "        y_pred = clf_cv.predict(x_test)    \n",
        "        acc_cv.append(accuracy_score(y_test, y_pred))\n",
        "\n",
        "    acc_hyp = np.mean(acc_cv)\n",
        "    acc.append(acc_hyp)\n",
        "    print('ACC:', acc_hyp)\n",
        "\n",
        "opt_index = np.argmax(acc)\n",
        "opt_hyperparameter = cc[opt_index]\n",
        "best_C = opt_hyperparameter\n",
        "print(\"Optimal C: \", best_C)\n",
        "\n",
        "plt.plot(cc, acc)\n",
        "plt.xscale('log')\n",
        "plt.xlabel(\"C\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"SVM - Accuracy vs. C\")\n",
        "plt.show()\n",
        "\n",
        "# Entrenar el modelo con el parametro optimo\n",
        "clf_svm = SVC(C=opt_hyperparameter, kernel='linear')\n",
        "clf_svm.fit(x, y)\n",
        "\n",
        "#------------------------------------------------------------------------------------------------------------------\n",
        "# Fin del archivo\n",
        "#------------------------------------------------------------------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prepara tus modelos para producción haciendo lo siguiente:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----- Modelos ajustados con todos los datos -----\n",
            "KNN Model: KNeighborsClassifier(n_neighbors=np.int64(13))\n",
            "SVM Model: SVC(C=np.float64(2.2570197196339215), kernel='linear')\n"
          ]
        }
      ],
      "source": [
        "# Ajustar el modelo KNN con todos los datos\n",
        "final_knn = KNeighborsClassifier(n_neighbors=best_k)\n",
        "final_knn.fit(x, y)\n",
        "\n",
        "# Ajustar el modelo SVM con todos los datos\n",
        "final_svm = SVC(C=best_C, kernel='linear')\n",
        "final_svm.fit(x, y)\n",
        "\n",
        "print(\"----- Modelos ajustados con todos los datos -----\")\n",
        "print(\"KNN Model:\", final_knn)\n",
        "print(\"SVM Model:\", final_svm)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----- Evaluación del Modelo KNN -----\n",
            "Precisión media con validación cruzada (KNN): 0.7248761904761906\n",
            "Desviación estándar de la precisión (KNN): 0.039120538958520724\n",
            "----- Evaluación del Modelo SVM -----\n",
            "Precisión media con validación cruzada (SVM): 0.7440126984126983\n",
            "Desviación estándar de la precisión (SVM): 0.022957887328257605\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Evaluar el modelo KNN con validacion cruzada\n",
        "knn_scores = cross_val_score(final_knn, x, y, cv=5, scoring='accuracy')\n",
        "print(\"----- Evaluacion del Modelo KNN -----\")\n",
        "print(f\"Precision media con validacion cruzada (KNN): {knn_scores.mean()}\")\n",
        "print(f\"Desviacion estandar de la precision (KNN): {knn_scores.std()}\")\n",
        "\n",
        "# Evaluar el modelo SVM con validacion cruzada\n",
        "svm_scores = cross_val_score(final_svm, x, y, cv=5, scoring='accuracy')\n",
        "print(\"----- Evaluacion del Modelo SVM -----\")\n",
        "print(f\"Precision media con validacion cruzada (SVM): {svm_scores.mean()}\")\n",
        "print(f\"Desviacion estandar de la precision (SVM): {svm_scores.std()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**¿Observas un problema en cuanto al balanceo de las clases? ¿Por qué?**\n",
        "\n",
        "No porque todas las clases tienen una cantidad igual de instancias, a excepcion de la ultima que es de 1 menos, pero no es necesario aplicar un balanceo.\n",
        "\n",
        "**¿Qué modelo o modelos fueron efectivos para clasificar tus datos? ¿Observas algo especial sobre los modelos? Argumenta tu respuesta.**\n",
        "\n",
        "SVM y KNN fueron efectivos y los que decidi utilizar. El SVM por ejemplo tiende a manejar bien los datos con multiples clases.\n",
        "\n",
        "**¿Observas alguna mejora importante al optimizar hiperparámetros?**\n",
        "\n",
        "El KNN si mejoro un poco al optimizar el valor de K, pero el SVM no mejoro mucho ya que inicialmente ya tenia un buen rendimiento\n",
        "\n",
        "**¿Qué inconvenientes hay al encontrar hiperparámetros? ¿Por qué?**\n",
        "\n",
        "Puede ser costoso computacionalmente y tardar algo de tiempo, y no en todos los casos mejora mucho como para que valga la pena encontrarlos."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "ZF-_9d2yWl__"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
